{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from bigru import BiGRU, train, predict\n",
    "\n",
    "GLOVE_MODEL_PATH = '../../csci544/hw4/glove.6B.100d'\n",
    "CLEAN_FINE_DATA_DIR = '../dataset/fine_data_clean.csv'\n",
    "LOAD_PRETRAINED_MODEL = False\n",
    "\n",
    "\n",
    "def generate_glove_vocab_embeddings(glove_path):\n",
    "    with open(glove_path,'rt') as f:\n",
    "        full_content = f.read().strip().split('\\n')\n",
    "\n",
    "    glove_dict, embeddings = {'<pad>': 0, '<eos>': 1, '<unk>': 2}, []\n",
    "    for line in full_content:\n",
    "        vals = line.split(' ')\n",
    "        word, embedding = vals[0], [float(x) for x in vals[1:]]\n",
    "        if word not in glove_dict:\n",
    "            glove_dict[word] = len(glove_dict)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # embeddings for special tokens: <pad>, <eos>, <unk>\n",
    "    pad_embedding = np.zeros((1, embeddings.shape[1]))\n",
    "    eos_embedding = np.ones((1, embeddings.shape[1]))\n",
    "    unk_embedding = np.mean(embeddings, axis=0, keepdims=True)\n",
    "\n",
    "    #insert embeddings for pad and unk tokens at top of embeddings.\n",
    "    embeddings = np.vstack((pad_embedding, eos_embedding, unk_embedding, embeddings))\n",
    "\n",
    "    return glove_dict, embeddings\n",
    "\n",
    "\n",
    "def get_word_index(word_dict, word):\n",
    "    if word in word_dict:\n",
    "        return word_dict[word]\n",
    "    # return <unk> if not in the training dictionary\n",
    "    return 2\n",
    "\n",
    "\n",
    "def convert_words(corpus, word_dict):\n",
    "    poems = []\n",
    "    for poem in corpus:\n",
    "        poem_vec = [get_word_index(word_dict, x) for x in str(poem).split(' ')]\n",
    "        poem_vec.append(1)\n",
    "        poems.append(poem_vec)\n",
    "\n",
    "    return poems\n",
    "\n",
    "def evaluate_score(y_true, y_pred):\n",
    "    print('----------- evalution ----------\\n',\n",
    "          'micro f1: %.6f\\n' % f1_score(y_true, y_pred, average='micro'),\n",
    "          'macro f1: %.6f\\n' % f1_score(y_true, y_pred, average='macro'),\n",
    "          'weighted f1: %.6f\\n' % f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#BATCH_SIZE = 32\n",
    "BATCH_SIZE = 4\n",
    "LOG_MODE = True\n",
    "\n",
    "\n",
    "\"\"\" NN model for classification \"\"\"\n",
    "\"\"\"\n",
    "    NN structure: GloVe Embedding -> Bi-GRU -> Multi-head Attention\n",
    "\"\"\"\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, output_num):\n",
    "        super(BiGRU, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 100\n",
    "        self.num_heads = 4\n",
    "        self.hidden_dim = 256\n",
    "        self.output_dim = 128\n",
    "        self.layer_num = 1\n",
    "        self.bi_num = 2\n",
    "\n",
    "        self.rnn = nn.GRU(self.embedding_dim, self.hidden_dim,\n",
    "                          bidirectional=True, batch_first=True)\n",
    "\n",
    "        #self.attention = nn.MultiheadAttention(self.hidden_dim, self.num_heads, batch_first=True)\n",
    "        #self.attn_fc_q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        #self.attn_fc_k = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        #self.attn_fc_v = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.33)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.elu = nn.ELU(alpha=1.0)\n",
    "\n",
    "        self.classifier = nn.Linear(self.output_dim, output_num)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.batch_size = len(input)\n",
    "        input_lens = [len(x) for x in input]\n",
    "        #mask = self.generate_mask(input_lens)\n",
    "\n",
    "        embeddings = [self.embedding(torch.tensor(x)) for x in input]\n",
    "        embedding_padded = pad_sequence(embeddings, batch_first=True)\n",
    "        embedding_padded = self.dropout(embedding_padded)\n",
    "        embedding_len = embedding_padded.shape[1]\n",
    "        embedding_packed = pack(embedding_padded, input_lens,\\\n",
    "                                batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # bi-rnn\n",
    "        hidden_0 = self.init_hidden(embedding_len)\n",
    "        output, _ = self.rnn(embedding_packed, hidden_0)\n",
    "        (output, output_len) = unpack(output, batch_first=True)\n",
    "\n",
    "        hiddens = output.chunk(2, dim=-1)\n",
    "        H = torch.mul(hiddens[0], hiddens[1])\n",
    "        output = H\n",
    "\n",
    "        # attention\n",
    "        #query = self.attn_fc_q(H)\n",
    "        #key = self.attn_fc_k(H)\n",
    "        #value = self.attn_fc_v(H)\n",
    "        #alpha, _ = self.attention(query, key, value)\n",
    "        #output = torch.mul(H, alpha)\n",
    "        #print('output: ', output.shape)\n",
    "\n",
    "        # attention\n",
    "        # ref: Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification\n",
    "        #M = torch.tanh(H)\n",
    "        #alpha = self.softmax(self.attn_fc(M))\n",
    "        #r = torch.mul(H, alpha)\n",
    "        #output = torch.tanh(r)\n",
    "        #print('output: ', output.shape)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        output = self.elu(output)\n",
    "        output = self.classifier(output)\n",
    "        return (output, output_len)\n",
    "\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.kaiming_uniform_(self.rnn.weight_ih_l0.data)\n",
    "        nn.init.kaiming_uniform_(self.rnn.weight_hh_l0.data)\n",
    "        #nn.init.xavier_uniform_(self.attn_fc.weight.data)\n",
    "        #nn.init.xavier_uniform_(self.attn_fc_q.weight.data)\n",
    "        #nn.init.xavier_uniform_(self.attn_fc_k.weight.data)\n",
    "        #nn.init.xavier_uniform_(self.attn_fc_v.weight.data)\n",
    "        nn.init.xavier_uniform_(self.linear.weight.data)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight.data)\n",
    "\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0.data, 0)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0.data, 0)\n",
    "        #nn.init.constant_(self.attn_fc.bias.data, 0)\n",
    "        #nn.init.constant_(self.attn_fc_q.bias.data, 0)\n",
    "        #nn.init.constant_(self.attn_fc_k.bias.data, 0)\n",
    "        #nn.init.constant_(self.attn_fc_v.bias.data, 0)\n",
    "        nn.init.constant_(self.linear.bias.data, 0)\n",
    "        nn.init.constant_(self.classifier.bias.data, 0)\n",
    "\n",
    "    def init_hidden(self, input_dim):\n",
    "        #return (torch.zeros(self.layer_num*self.bi_num, self.batch_size, self.hidden_dim),\n",
    "        #        torch.zeros(self.layer_num*self.bi_num, self.batch_size, self.hidden_dim))\n",
    "        return torch.zeros(self.layer_num*self.bi_num, self.batch_size, self.hidden_dim)\n",
    "\n",
    "    # init embedding with GloVe\n",
    "    def init_embedding(self, embeddings):\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float())\n",
    "\n",
    "\n",
    "    def generate_mask(self, lens):\n",
    "        max_len = np.max(lens)\n",
    "        mask = torch.zeros(len(lens), max_len)\n",
    "        for i in range(len(lens)):\n",
    "            mask[i][:lens[i]] = 1\n",
    "        return mask\n",
    "\n",
    "\n",
    "def batch_data(X):\n",
    "    X_batch = []\n",
    "    for start in range(0, len(X), BATCH_SIZE):\n",
    "        X_batch.append(X[start : start+BATCH_SIZE])\n",
    "    return X_batch\n",
    "\n",
    "\n",
    "def train(model, model_dir, X, y, criterion, optimizer, n_epochs=5):\n",
    "    # prepare input data\n",
    "    X_train, X_valid, y_train, y_valid = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "\n",
    "    # batch data\n",
    "    X_train_batch, y_train_batch = batch_data(X_train), batch_data(y_train)\n",
    "    X_valid_batch, y_valid_batch = batch_data(X_valid), batch_data(y_valid)\n",
    "\n",
    "    #print(X_train_batch)\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.process_time()\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # train the model\n",
    "        model.train() # prep model for training\n",
    "        for X, target in zip(X_train_batch, y_train_batch):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output, output_len = model(X)\n",
    "            N = output.size(0)\n",
    "            predictions = torch.stack([output[i][output_len[i]-1].float() for i in range(N)])\n",
    "            targets = torch.LongTensor(target)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(predictions, targets) / N\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*torch.max(output_len).item()\n",
    "\n",
    "            \n",
    "        # validate the model\n",
    "        model.eval() # prep model for evaluation\n",
    "        for X, target in zip(X_valid_batch, y_valid_batch):\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output, output_len = model(X)\n",
    "            N = output.size(0)\n",
    "            predictions = torch.stack([output[i][output_len[i]-1].float() for i in range(N)])\n",
    "            targets = torch.LongTensor(target)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(predictions, targets) / N\n",
    "            # update running validation loss\n",
    "            valid_loss += loss.item()*torch.max(output_len).item()\n",
    "\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(X_train_batch)\n",
    "        valid_loss = valid_loss / len(X_valid_batch)\n",
    "\n",
    "\n",
    "        if LOG_MODE:\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch+1,\n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            if LOG_MODE:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                        valid_loss_min,\n",
    "                        valid_loss))\n",
    "            torch.save(model.state_dict(), model_dir)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        end = time.process_time()\n",
    "        if LOG_MODE:\n",
    "            print('time each epoch: %.2f s' % (end-start))\n",
    "\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X):\n",
    "    X_batch = batch_data(X)\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    y_pred = []\n",
    "    for batch_idx in range(len(X_batch)):\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output, output_len = model(X_batch[batch_idx])\n",
    "        for sentence_idx in range(len(output)):\n",
    "            prediction = output[sentence_idx][output_len[sentence_idx]-1].float()\n",
    "            y_pred.append(torch.argmax(softmax(prediction), dim=0))\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(CLEAN_FINE_DATA_DIR).sample(frac=1.0, random_state=19).reset_index(drop=True)[:1000]\n",
    "poems, labels = raw_data['poem'].to_numpy(), raw_data['label'].to_numpy(dtype=int)\n",
    "label_num = len(np.unique(labels))\n",
    "#print(poems, len(poems))\n",
    "#print(labels, len(labels))\n",
    "\n",
    "glove_dict, embeddings = generate_glove_vocab_embeddings(GLOVE_MODEL_PATH)\n",
    "#print(glove_dict)\n",
    "\n",
    "\n",
    "X = convert_words(poems, glove_dict)\n",
    "#print(X)\n",
    "#print(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=23)\n",
    "#print(X_train, y_train)\n",
    "\n",
    "model = BiGRU(label_num)\n",
    "model.init_embedding(embeddings)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer and learning rate\n",
    "#learning_rate = 1e-4\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print('learning_rate =', learning_rate)\n",
    "\n",
    "# train RNN\n",
    "n_epochs = 10\n",
    "if not LOAD_PRETRAINED_MODEL:\n",
    "    bigru = train(model, 'bigru.pt',\n",
    "                   X_train, y_train,\n",
    "                   criterion, optimizer, n_epochs=n_epochs)\n",
    "else:\n",
    "    bigru = model\n",
    "    bigru.load_state_dict(torch.load('bigru.pt'))\n",
    "\n",
    "bigru.eval()\n",
    "y_pred = predict(bigru, X_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_test[i] != y_pred[i].item():\n",
    "        print(y_test[i], y_pred[i])\n",
    "evaluate_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
